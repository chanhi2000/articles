import{_ as h}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as g,b as n,t as v,e as s,n as k,g as f,w as a,d as e,f as l,r as i,o as b}from"./app-BgNevrm5.js";const y={},w={id:"frontmatter-title-관련",tabindex:"-1"},x={class:"header-anchor",href:"#frontmatter-title-관련"},_=n("p",null,"Large Language Models are everywhere these days – think ChatGPT – but they have their fair share of challenges.",-1),A=n("p",null,"One of the biggest challenges faced by LLMs is hallucination. This occurs when the model generates text that is factually incorrect or misleading, often based on patterns it has learned from its training data. So how can Retrieval-Augmented Generation, or RAG, help mitigate this issue?",-1),q=n("p",null,"By retrieving relevant information from a more vast, wider knowledge base, RAG ensures that the LLM's responses are grounded in real-world facts. This significantly reduces the likelihood of hallucinations and improves the overall accuracy and reliability of the generated content.",-1),I=n("hr",null,null,-1),R=n("h2",{id:"table-of-contents",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#table-of-contents"},[n("span",null,"Table of Contents")])],-1),T={class:"table-of-contents"},L=l('<hr><h2 id="what-is-retrieval-augmented-generation-rag" tabindex="-1"><a class="header-anchor" href="#what-is-retrieval-augmented-generation-rag"><span>What is Retrieval Augmented Generation (RAG)?</span></a></h2><p>RAG is a technique that combines information retrieval with language generation. Think of it as a two-step process:</p><ol><li><strong>Retrieval:</strong> The model first retrieves relevant information from a large corpus of documents based on the user&#39;s query.</li><li><strong>Generation:</strong> Using this retrieved information, the model then generates a comprehensive and informative response.</li></ol><h3 id="why-use-llamaindex-for-rag" tabindex="-1"><a class="header-anchor" href="#why-use-llamaindex-for-rag"><span>Why use LlamaIndex for RAG?</span></a></h3><p>LlamaIndex is a powerful framework that simplifies the process of building RAG pipelines. It provides a flexible and efficient way to connect retrieval components (like vector databases and embedding models) with generation components (like LLMs).</p><h3 id="some-of-the-key-benefits-of-using-llama-index-include" tabindex="-1"><a class="header-anchor" href="#some-of-the-key-benefits-of-using-llama-index-include"><span>Some of the key benefits of using Llama-Index include:</span></a></h3><ul><li><strong>Modularity:</strong> It allows you to easily customize and experiment with different components.</li><li><strong>Scalability:</strong> It can handle large datasets and complex queries.</li><li><strong>Ease of use:</strong> It provides a high-level API that abstracts away much of the underlying complexity.</li></ul><h3 id="what-you-ll-learn-here" tabindex="-1"><a class="header-anchor" href="#what-you-ll-learn-here"><span>What You&#39;ll Learn Here:</span></a></h3><p>In this article, we will delve deeper into the components of a RAG pipeline and explore how you can use LlamaIndex to build these systems.</p><p>We will cover topics such as vector databases, embedding models, language models, and the role of LlamaIndex in connecting these components.</p><hr><h2 id="understanding-the-components-of-a-rag-pipeline" tabindex="-1"><a class="header-anchor" href="#understanding-the-components-of-a-rag-pipeline"><span>Understanding the Components of a RAG Pipeline</span></a></h2><p>Here&#39;s a diagram that&#39;ll help familiarize you with the basics of RAG architecture:</p><figure><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1724944925051/e525c6cb-6a99-4eec-8b47-3dc827ddff25.png" alt="RAG Architecture showing the flow from the user query through to the response" tabindex="0" loading="lazy"><figcaption>RAG Architecture showing the flow from the user query through to the response</figcaption></figure>',15),G={href:"https://fivetran.com/blog/assembling-a-rag-architecture-using-fivetran",target:"_blank",rel:"noopener noreferrer"},P=n("h3",{id:"components-of-rag",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#components-of-rag"},[n("span",null,"Components of RAG")])],-1),S=n("ul",null,[n("li",null,[n("strong",null,"Vector Databases:"),e(" These databases are optimized for storing and searching high-dimensional vectors. They are crucial for efficiently finding relevant information from a vast corpus of documents.")]),n("li",null,[n("strong",null,"Embedding Models:"),e(" These models convert text into numerical representations or embeddings. These embeddings capture the semantic meaning of the text, allowing for efficient comparison and retrieval in vector databases.")])],-1),C=n("p",null,"A vector is a mathematical object that represents a quantity with both magnitude (size) and direction. In the context of RAG, embeddings are high-dimensional vectors that capture the semantic meaning of text. Each dimension of the vector represents a different aspect of the text's meaning, allowing for efficient comparison and retrieval.",-1),D=n("ul",null,[n("li",null,[n("strong",null,"Language Models:"),e(" These models are trained on massive amounts of text data, enabling them to generate human-quality text. They are capable of understanding and responding to prompts in a coherent and informative manner.")])],-1),B=l('<h3 id="the-rag-flow" tabindex="-1"><a class="header-anchor" href="#the-rag-flow"><span>The RAG Flow</span></a></h3><ol><li><strong>Query Submission:</strong> A user submits a query or question.</li><li><strong>Embedding Creation:</strong> The query is converted into an embedding using the same embedding model used for the corpus.</li><li><strong>Retrieval:</strong> The embedding is searched against the vector database to find the most relevant documents.</li><li><strong>Contextualization:</strong> The retrieved documents are combined with the original query to form a context.</li><li><strong>Generation:</strong> The language model generates a response based on the provided context.</li></ol><h3 id="lamaindex" tabindex="-1"><a class="header-anchor" href="#lamaindex"><span>LamaIndex</span></a></h3><p>LlamaIndex plays a crucial role in connecting the retrieval and generation components. It acts as an index that maps queries to relevant documents. By efficiently managing the index, LlamaIndex ensures that the retrieval process is fast and accurate.</p><hr><h2 id="prerequisites" tabindex="-1"><a class="header-anchor" href="#prerequisites"><span>Prerequisites</span></a></h2>',6),E={href:"https://ibm.com/products/watsonx-ai",target:"_blank",rel:"noopener noreferrer"},M=n("li",null,"Python 3.9+",-1),W={href:"https://dataplatform.cloud.ibm.com/docs/content/wsj/admin/admin-apikeys.html?context=wx",target:"_blank",rel:"noopener noreferrer"},H=n("li",null,"Curiosity to learn",-1),j=l(`<hr><h2 id="let-s-get-started" tabindex="-1"><a class="header-anchor" href="#let-s-get-started"><span>Let&#39;s Get Started!</span></a></h2><p>In this article, we will be using LlamaIndex to make a simple RAG Pipeline.</p><p>Let&#39;s create a virtual environment for Python using the following command in your terminal: <code>python -m venv venv</code> . This will create a virtual environment (venv) for your project. If you are a Windows user you can activate it using <code>.\\venv\\Scripts\\activate</code>, and Mac users can activate it with <code>source venv/bin/activate</code>.</p><p>Now let&#39;s install the packages:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">pip <span class="token function">install</span> wikipedia llama-index-llms-ibm llama-index-embeddings-huggingface</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Once these packages are installed, you will need watsonx.ai&#39;s API key as well. This in turn will help you use LLMs via LlamaIndex.</p>`,7),z={href:"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui",target:"_blank",rel:"noopener noreferrer"},O=l(`<div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">import</span> wikipedia</span>
<span class="line"></span>
<span class="line"><span class="token comment"># Search for a specific page</span></span>
<span class="line">page <span class="token operator">=</span> wikipedia<span class="token punctuation">.</span>page<span class="token punctuation">(</span><span class="token string">&quot;Artificial Intelligence&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Access the content</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>page<span class="token punctuation">.</span>content<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Now let&#39;s save the page content to a text document. We are doing it so that we can access it later. You can do this using the below code:</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">import</span> os</span>
<span class="line"></span>
<span class="line"><span class="token comment"># Create the &#39;Document&#39; directory if it doesn&#39;t exist</span></span>
<span class="line"><span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token string">&#39;Document&#39;</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span><span class="token string">&#39;Document&#39;</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Open the file &#39;AI.txt&#39; in write mode with UTF-8 encoding</span></span>
<span class="line"><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&#39;Document/AI.txt&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;w&#39;</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">&#39;utf-8&#39;</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span></span>
<span class="line">    <span class="token comment"># Write the content of the &#39;page&#39; object to the file</span></span>
<span class="line">    f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>page<span class="token punctuation">.</span>content<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Now we&#39;ll be using watsonx.ai via LlamaIndex. It will help us generate responses based on the user&#39;s query.</p><p>Note: Make sure to replace the parameters <code>WATSONX_APIKEY</code> and <code>project_id</code> with your values in the below code:</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">import</span> os</span>
<span class="line"><span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>ibm <span class="token keyword">import</span> WatsonxLLM</span>
<span class="line"><span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core <span class="token keyword">import</span> SimpleDirectoryReader<span class="token punctuation">,</span> Document</span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Define a function to generate responses using the WatsonxLLM instance</span></span>
<span class="line"><span class="token keyword">def</span> <span class="token function">generate_response</span><span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token triple-quoted-string string">&quot;&quot;&quot;</span>
<span class="line">    Generates a response to the given prompt using the WatsonxLLM instance.</span>
<span class="line"></span>
<span class="line">    Args:</span>
<span class="line">        prompt (str): The prompt to provide to the large language model.</span>
<span class="line"></span>
<span class="line">    Returns:</span>
<span class="line">        str: The generated response from the WatsonxLLM.</span>
<span class="line">    &quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line">    response <span class="token operator">=</span> watsonx_llm<span class="token punctuation">.</span>complete<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span></span>
<span class="line">    <span class="token keyword">return</span> response</span>
<span class="line"></span>
<span class="line"><span class="token comment"># Set the WATSONX_APIKEY environment variable (replace with your actual key)</span></span>
<span class="line">os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;WATSONX_APIKEY&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">&#39;YOUR_WATSONX_APIKEY&#39;</span>  <span class="token comment"># Replace with your API key</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Define model parameters (adjust as needed)</span></span>
<span class="line">temperature <span class="token operator">=</span> <span class="token number">0</span></span>
<span class="line">max_new_tokens <span class="token operator">=</span> <span class="token number">1500</span></span>
<span class="line">additional_params <span class="token operator">=</span> <span class="token punctuation">{</span></span>
<span class="line">    <span class="token string">&quot;decoding_method&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;sample&quot;</span><span class="token punctuation">,</span></span>
<span class="line">    <span class="token string">&quot;min_new_tokens&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span></span>
<span class="line">    <span class="token string">&quot;top_k&quot;</span><span class="token punctuation">:</span> <span class="token number">50</span><span class="token punctuation">,</span></span>
<span class="line">    <span class="token string">&quot;top_p&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Create a WatsonxLLM instance with the specified model, URL, project ID, and parameters</span></span>
<span class="line">watsonx_llm <span class="token operator">=</span> WatsonxLLM<span class="token punctuation">(</span></span>
<span class="line">    model_id<span class="token operator">=</span><span class="token string">&quot;meta-llama/llama-3-1-70b-instruct&quot;</span><span class="token punctuation">,</span></span>
<span class="line">    url<span class="token operator">=</span><span class="token string">&quot;https://us-south.ml.cloud.ibm.com&quot;</span><span class="token punctuation">,</span></span>
<span class="line">    project_id<span class="token operator">=</span><span class="token string">&quot;YOUR_PROJECT_ID&quot;</span><span class="token punctuation">,</span></span>
<span class="line">    temperature<span class="token operator">=</span>temperature<span class="token punctuation">,</span></span>
<span class="line">    max_new_tokens<span class="token operator">=</span>max_new_tokens<span class="token punctuation">,</span></span>
<span class="line">    additional_params<span class="token operator">=</span>additional_params<span class="token punctuation">,</span></span>
<span class="line"><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Load documents from the specified directory</span></span>
<span class="line">documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span></span>
<span class="line">    input_files<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;Document/AI.txt&quot;</span><span class="token punctuation">]</span></span>
<span class="line"><span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Combine the text content of all documents into a single Document object</span></span>
<span class="line">combined_documents <span class="token operator">=</span> Document<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token string">&quot;\\n\\n&quot;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>doc<span class="token punctuation">.</span>text <span class="token keyword">for</span> doc <span class="token keyword">in</span> documents<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Print the combined document</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>combined_documents<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Here&#39;s a breakdown of the parameters:</p><ul><li><strong>temperature = 0:</strong> This setting makes the model generate the most likely text sequence, leading to a more deterministic and predictable output. It&#39;s like telling the model to stick to the most common words and phrases.</li><li><strong>max_new_tokens = 1500:</strong> This limits the generated text to a maximum of 1500 new tokens (words or parts of words).</li><li><strong>additional_params:</strong><ul><li><strong>decoding_method = &quot;sample&quot;:</strong> This means the model will generate text randomly based on the probability distribution of each token.</li><li><strong>min_new_tokens = 1:</strong> Ensures that at least one new token is generated, preventing the model from repeating itself.</li><li><strong>top_k = 50:</strong> This limits the model&#39;s choices to the 50 most likely tokens at each step, making the output more focused and less random.</li><li><strong>top_p = 1:</strong> This sets the nucleus sampling probability to 1, meaning all tokens with a probability greater than or equal to the top_p value will be considered.</li></ul></li></ul><p>You can tweak these parameters for experimentation and see how they affect your response. Now we&#39;ll be building and loading a vector store index from the given document. But first, let&#39;s understand what it is.</p><h3 id="understanding-vector-store-indexes" tabindex="-1"><a class="header-anchor" href="#understanding-vector-store-indexes"><span>Understanding Vector Store Indexes</span></a></h3><p>A vector store index is a specialized data structure designed to efficiently store and retrieve high-dimensional vectors. In the context of the Llama Index, these vectors represent the semantic embeddings of documents.</p><h4 id="key-characteristics-of-vector-store-indexes" tabindex="-1"><a class="header-anchor" href="#key-characteristics-of-vector-store-indexes"><span>Key characteristics of vector store indexes:</span></a></h4><ul><li><strong>High-dimensional vectors:</strong> Each document is represented as a high-dimensional vector, capturing its semantic meaning.</li><li><strong>Efficient retrieval:</strong> Vector store indexes are optimized for fast similarity search, allowing you to quickly find documents that are semantically similar to a given query.</li><li><strong>Scalability:</strong> They can handle large datasets and scale efficiently as the number of documents grows.</li></ul><h4 id="how-llama-index-uses-vector-store-indexes" tabindex="-1"><a class="header-anchor" href="#how-llama-index-uses-vector-store-indexes"><span>How Llama Index uses vector store indexes:</span></a></h4><ol><li><strong>Document Embedding:</strong> Documents are first converted into high-dimensional vectors using a language model like Llama.</li><li><strong>Index Creation:</strong> The embeddings are stored in a vector store index.</li><li><strong>Query Processing:</strong> When a user submits a query, it is also converted into a vector. The vector store index is then used to find the most similar documents based on their embeddings.</li><li><strong>Response Generation:</strong> The retrieved documents are used to generate a relevant response.</li></ol><p>In the below code, you&#39;ll come across the word &quot;chunk&quot;. <strong>A chunk</strong> is a smaller, manageable unit of text extracted from a larger document. It&#39;s typically a paragraph or a few sentences long. They are used to make the retrieval and processing of information more efficient, especially when dealing with large documents.</p><p>By breaking down documents into chunks, RAG systems can focus on the most relevant parts and generate more accurate and concise responses.</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core<span class="token punctuation">.</span>node_parser <span class="token keyword">import</span> SentenceSplitter</span>
<span class="line"><span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core <span class="token keyword">import</span> VectorStoreIndex<span class="token punctuation">,</span> load_index_from_storage</span>
<span class="line"><span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core <span class="token keyword">import</span> Settings</span>
<span class="line"><span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core <span class="token keyword">import</span> StorageContext</span>
<span class="line"></span>
<span class="line"><span class="token keyword">def</span> <span class="token function">get_build_index</span><span class="token punctuation">(</span>documents<span class="token punctuation">,</span> embed_model<span class="token operator">=</span><span class="token string">&quot;local:BAAI/bge-small-en-v1.5&quot;</span><span class="token punctuation">,</span> save_dir<span class="token operator">=</span><span class="token string">&quot;./vector_store/index&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token triple-quoted-string string">&quot;&quot;&quot;</span>
<span class="line">    Builds or loads a vector store index from the given documents.</span>
<span class="line"></span>
<span class="line">    Args:</span>
<span class="line">        documents (list[Document]): A list of Document objects.</span>
<span class="line">        embed_model (str, optional): The embedding model to use. Defaults to &quot;local:BAAI/bge-small-en-v1.5&quot;.</span>
<span class="line">        save_dir (str, optional): The directory to save or load the index from. Defaults to &quot;./vector_store/index&quot;.</span>
<span class="line"></span>
<span class="line">    Returns:</span>
<span class="line">        VectorStoreIndex: The built or loaded index.</span>
<span class="line">    &quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line">    <span class="token comment"># Set index settings</span></span>
<span class="line">    Settings<span class="token punctuation">.</span>llm <span class="token operator">=</span> watsonx_llm</span>
<span class="line">    Settings<span class="token punctuation">.</span>embed_model <span class="token operator">=</span> embed_model</span>
<span class="line">    Settings<span class="token punctuation">.</span>node_parser <span class="token operator">=</span> SentenceSplitter<span class="token punctuation">(</span>chunk_size<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span></span>
<span class="line">    Settings<span class="token punctuation">.</span>num_output <span class="token operator">=</span> <span class="token number">512</span></span>
<span class="line">    Settings<span class="token punctuation">.</span>context_window <span class="token operator">=</span> <span class="token number">3900</span></span>
<span class="line"></span>
<span class="line">    <span class="token comment"># Check if the save directory exists</span></span>
<span class="line">    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>save_dir<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token comment"># Create and load the index</span></span>
<span class="line">        index <span class="token operator">=</span> VectorStoreIndex<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span></span>
<span class="line">            <span class="token punctuation">[</span>documents<span class="token punctuation">]</span><span class="token punctuation">,</span> service_context<span class="token operator">=</span>Settings</span>
<span class="line">        <span class="token punctuation">)</span></span>
<span class="line">        index<span class="token punctuation">.</span>storage_context<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>persist_dir<span class="token operator">=</span>save_dir<span class="token punctuation">)</span></span>
<span class="line">    <span class="token keyword">else</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token comment"># Load the existing index</span></span>
<span class="line">        index <span class="token operator">=</span> load_index_from_storage<span class="token punctuation">(</span></span>
<span class="line">            StorageContext<span class="token punctuation">.</span>from_defaults<span class="token punctuation">(</span>persist_dir<span class="token operator">=</span>save_dir<span class="token punctuation">)</span><span class="token punctuation">,</span></span>
<span class="line">            service_context<span class="token operator">=</span>Settings<span class="token punctuation">,</span></span>
<span class="line">        <span class="token punctuation">)</span></span>
<span class="line">    <span class="token keyword">return</span> index</span>
<span class="line"></span>
<span class="line"><span class="token comment"># Get the Vector Index</span></span>
<span class="line">vector_index <span class="token operator">=</span> get_build_index<span class="token punctuation">(</span>documents<span class="token operator">=</span>documents<span class="token punctuation">,</span> embed_model<span class="token operator">=</span><span class="token string">&quot;local:BAAI/bge-small-en-v1.5&quot;</span><span class="token punctuation">,</span> save_dir<span class="token operator">=</span><span class="token string">&quot;./vector_store/index&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>This is the last part of RAG: we create a query engine with metadata replacement and sentence transformer reranking. Bruh! What is a re-ranker now?</p><p><strong>A re-ranker</strong> is a component that reorders the retrieved documents based on their relevance to the query. It uses additional information, such as semantic similarity or context-specific factors, to refine the initial ranking provided by the retrieval system. This helps ensure that the most relevant documents are presented to the user, leading to more accurate and informative responses.</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core<span class="token punctuation">.</span>postprocessor <span class="token keyword">import</span> MetadataReplacementPostProcessor<span class="token punctuation">,</span> SentenceTransformerRerank</span>
<span class="line"></span>
<span class="line"><span class="token keyword">def</span> <span class="token function">get_query_engine</span><span class="token punctuation">(</span>sentence_index<span class="token punctuation">,</span> similarity_top_k<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> rerank_top_n<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token triple-quoted-string string">&quot;&quot;&quot;</span>
<span class="line">    Creates a query engine with metadata replacement and sentence transformer reranking.</span>
<span class="line"></span>
<span class="line">    Args:</span>
<span class="line">        sentence_index (VectorStoreIndex): The sentence index to use.</span>
<span class="line">        similarity_top_k (int, optional): The number of similar nodes to consider. Defaults to 6.</span>
<span class="line">        rerank_top_n (int, optional): The number of nodes to rerank. Defaults to 2.</span>
<span class="line"></span>
<span class="line">    Returns:</span>
<span class="line">        QueryEngine: The query engine.</span>
<span class="line">    &quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line">    postproc <span class="token operator">=</span> MetadataReplacementPostProcessor<span class="token punctuation">(</span>target_metadata_key<span class="token operator">=</span><span class="token string">&quot;window&quot;</span><span class="token punctuation">)</span></span>
<span class="line">    rerank <span class="token operator">=</span> SentenceTransformerRerank<span class="token punctuation">(</span></span>
<span class="line">        top_n<span class="token operator">=</span>rerank_top_n<span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">&quot;BAAI/bge-reranker-base&quot;</span></span>
<span class="line">    <span class="token punctuation">)</span></span>
<span class="line">    engine <span class="token operator">=</span> sentence_index<span class="token punctuation">.</span>as_query_engine<span class="token punctuation">(</span></span>
<span class="line">        similarity_top_k<span class="token operator">=</span>similarity_top_k<span class="token punctuation">,</span> node_postprocessors<span class="token operator">=</span><span class="token punctuation">[</span>postproc<span class="token punctuation">,</span> rerank<span class="token punctuation">]</span></span>
<span class="line">    <span class="token punctuation">)</span></span>
<span class="line">    <span class="token keyword">return</span> engine</span>
<span class="line"></span>
<span class="line"><span class="token comment"># Create a query engine with the specified parameters</span></span>
<span class="line">query_engine <span class="token operator">=</span> get_query_engine<span class="token punctuation">(</span>sentence_index<span class="token operator">=</span>vector_index<span class="token punctuation">,</span> similarity_top_k<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> rerank_top_n<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Query the engine with a question</span></span>
<span class="line">query <span class="token operator">=</span> <span class="token string">&#39;What is Deep learning?&#39;</span></span>
<span class="line">response <span class="token operator">=</span> query_engine<span class="token punctuation">.</span>query<span class="token punctuation">(</span>query<span class="token punctuation">)</span></span>
<span class="line">prompt <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f&#39;&#39;&#39;Generate a detailed response for the query asked based only on the context fetched:</span>
<span class="line">            Query: </span><span class="token interpolation"><span class="token punctuation">{</span>query<span class="token punctuation">}</span></span><span class="token string"></span>
<span class="line">            Context: </span><span class="token interpolation"><span class="token punctuation">{</span>response<span class="token punctuation">}</span></span><span class="token string"></span>
<span class="line"></span>
<span class="line">            Instructions:</span>
<span class="line">            1. Show query and your generated response based on context.</span>
<span class="line">            2. Your response should be detailed and should cover every aspect of the context.</span>
<span class="line">            3. Be crisp and concise.</span>
<span class="line">            4. Don&#39;t include anything else in your response - no header/footer/code etc</span>
<span class="line">            &#39;&#39;&#39;</span></span></span>
<span class="line">response <span class="token operator">=</span> generate_response<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token triple-quoted-string string">&#39;&#39;&#39;</span>
<span class="line">OUTPUT - </span>
<span class="line">Query: What is Deep learning? </span>
<span class="line"></span>
<span class="line">Deep learning is a subset of artificial intelligence that utilizes multiple layers of neurons between the network&#39;s inputs and outputs to progressively extract higher-level features from raw input data. </span>
<span class="line">This technique allows for improved performance in various subfields of AI, such as computer vision, speech recognition, natural language processing, and image classification. </span>
<span class="line">The multiple layers in deep learning networks are able to identify complex concepts and patterns, including edges, faces, digits, and letters.</span>
<span class="line">The reason behind deep learning&#39;s success is not attributed to a recent theoretical breakthrough, but rather the significant increase in computer power, particularly the shift to using graphics processing units (GPUs), which provided a hundred-fold increase in speed. </span>
<span class="line">Additionally, the availability of vast amounts of training data, including large curated datasets, has also contributed to the success of deep learning.</span>
<span class="line">Overall, deep learning&#39;s ability to analyze and extract insights from raw data has led to its widespread application in various fields, and its performance continues to improve with advancements in technology and data availability. &#39;&#39;&#39;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><h2 id="how-to-fine-tune-the-pipeline" tabindex="-1"><a class="header-anchor" href="#how-to-fine-tune-the-pipeline"><span>How to Fine-Tune the Pipeline</span></a></h2><p>Once you&#39;ve built a basic RAG pipeline, the next step is to fine-tune it for optimal performance. This involves iteratively adjusting various components and parameters to improve the quality of the generated responses.</p><h3 id="how-to-evaluate-the-pipeline-s-performance" tabindex="-1"><a class="header-anchor" href="#how-to-evaluate-the-pipeline-s-performance"><span>How to Evaluate the Pipeline&#39;s Performance</span></a></h3><p>To assess the pipeline&#39;s effectiveness, you can use <strong>metrics</strong> like:</p><ul><li><strong>Accuracy:</strong> How often does the pipeline generate correct and relevant responses?</li><li><strong>Relevance:</strong> How well do the retrieved documents match the query?</li><li><strong>Coherence:</strong> Is the generated text well-structured and easy to understand?</li><li><strong>Factuality:</strong> Are the generated responses accurate and consistent with known facts?</li></ul><h3 id="iterate-on-the-index-structure-embedding-model-and-language-model" tabindex="-1"><a class="header-anchor" href="#iterate-on-the-index-structure-embedding-model-and-language-model"><span>Iterate on the Index Structure, Embedding Model, and Language Model</span></a></h3><p>You can experiment with different <strong>index structures</strong> (for example flat index, hierarchical index) to find the one that best suits your data and query patterns. Consider using <strong>different embedding models</strong> to capture different semantic nuances. <strong>Fine-tuning the language model</strong> can also improve its ability to generate high-quality responses.</p><h3 id="experiment-with-different-hyperparameters" tabindex="-1"><a class="header-anchor" href="#experiment-with-different-hyperparameters"><span>Experiment with Different Hyperparameters</span></a></h3><p><strong>Hyperparameters</strong> are settings that control the behaviour of the pipeline components. By experimenting with different values, you can optimize the pipeline&#39;s performance. Some examples of hyperparameters include:</p><ul><li><strong>Embedding dimension:</strong> The size of the embedding vectors</li><li><strong>Index size:</strong> The maximum number of documents to store in the index</li><li><strong>Retrieval threshold:</strong> The minimum similarity score for a document to be considered relevant</li></ul><hr><h2 id="real-world-applications-of-rag" tabindex="-1"><a class="header-anchor" href="#real-world-applications-of-rag"><span>Real-World Applications of RAG</span></a></h2><p>RAG pipelines have a wide range of applications, including:</p><ul><li><strong>Customer support chatbots:</strong> Providing informative and helpful responses to customer inquiries</li><li><strong>Knowledge base search:</strong> Efficiently retrieving relevant information from large document collections</li><li><strong>Summarization of large documents:</strong> Condensing lengthy documents into concise summaries</li><li><strong>Question answering systems:</strong> Answering complex questions based on a given corpus of knowledge</li></ul><hr><h2 id="rag-best-practices-and-considerations" tabindex="-1"><a class="header-anchor" href="#rag-best-practices-and-considerations"><span>RAG Best Practices and Considerations</span></a></h2><p>To build effective RAG pipelines, consider these best practices:</p><ul><li><strong>Data quality and preprocessing:</strong> Ensure your data is clean, consistent, and relevant to your use case. Preprocess the data to remove noise and improve its quality.</li><li><strong>Embedding model selection:</strong> Choose an embedding model that is appropriate for your specific domain and task. Consider factors like accuracy, computational efficiency, and interpretability.</li><li><strong>Index optimization:</strong> Optimize the index structure and parameters to improve retrieval efficiency and accuracy.</li><li><strong>Ethical considerations and biases:</strong> Be aware of potential biases in your data and models. Take steps to mitigate bias and ensure fairness in your RAG pipeline.</li></ul><hr><h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="#conclusion"><span>Conclusion</span></a></h2><p>RAG pipelines offer a powerful approach to leveraging large language models for a variety of tasks. By carefully selecting and fine-tuning the components of an RAG pipeline, you can build systems that provide informative, accurate, and relevant responses.</p><h3 id="key-points-to-remember" tabindex="-1"><a class="header-anchor" href="#key-points-to-remember"><span>Key points to remember:</span></a></h3><ul><li>RAG combines information retrieval and language generation.</li><li>Llama-Index simplifies the process of building RAG pipelines.</li><li>Fine-tuning is essential for optimizing pipeline performance.</li><li>RAG has a wide range of real-world applications.</li><li>Ethical considerations are crucial in building responsible RAG systems.</li></ul><p>As RAG technology continues to evolve, we can expect to see even more innovative and powerful applications in the future. Till then, let&#39;s wait for the future to unfold!</p>`,46);function V(c,Y){const d=i("VPCard"),u=i("SiteInfo"),t=i("router-link"),o=i("FontIcon"),m=i("Tabs");return b(),g("div",null,[n("h1",w,[n("a",x,[n("span",null,v(c.$frontmatter.title)+" 관련",1)])]),s(d,k(f({title:"Llama > Article(s)",desc:"Article(s)",link:"/ai/llama/articles/README.md",logo:"/images/ico-wind.svg",background:"rgba(10,10,10,0.2)"})),null,16),s(u,{name:"How to Build a RAG Pipeline with LlamaIndex",desc:"Large Language Models are everywhere these days – think ChatGPT – but they have their fair share of challenges. One of the biggest challenges faced by LLMs is hallucination. This occurs when the model generates text that is factually incorrect or mis...",url:"https://freecodecamp.org/news/how-to-build-a-rag-pipeline-with-llamaindex/",logo:"https://cdn.freecodecamp.org/universal/favicons/favicon.ico",preview:"https://cdn.hashnode.com/res/hashnode/image/upload/v1725024307257/62401eea-25ab-4f00-93d7-76d7c49cf330.jpeg"}),_,A,q,I,R,n("nav",T,[n("ul",null,[n("li",null,[s(t,{to:"#table-of-contents"},{default:a(()=>[e("Table of Contents")]),_:1})]),n("li",null,[s(t,{to:"#what-is-retrieval-augmented-generation-rag"},{default:a(()=>[e("What is Retrieval Augmented Generation (RAG)?")]),_:1}),n("ul",null,[n("li",null,[s(t,{to:"#why-use-llamaindex-for-rag"},{default:a(()=>[e("Why use LlamaIndex for RAG?")]),_:1})]),n("li",null,[s(t,{to:"#some-of-the-key-benefits-of-using-llama-index-include"},{default:a(()=>[e("Some of the key benefits of using Llama-Index include:")]),_:1})]),n("li",null,[s(t,{to:"#what-you-ll-learn-here"},{default:a(()=>[e("What You'll Learn Here:")]),_:1})])])]),n("li",null,[s(t,{to:"#understanding-the-components-of-a-rag-pipeline"},{default:a(()=>[e("Understanding the Components of a RAG Pipeline")]),_:1}),n("ul",null,[n("li",null,[s(t,{to:"#components-of-rag"},{default:a(()=>[e("Components of RAG")]),_:1})]),n("li",null,[s(t,{to:"#the-rag-flow"},{default:a(()=>[e("The RAG Flow")]),_:1})]),n("li",null,[s(t,{to:"#lamaindex"},{default:a(()=>[e("LamaIndex")]),_:1})])])]),n("li",null,[s(t,{to:"#prerequisites"},{default:a(()=>[e("Prerequisites")]),_:1})]),n("li",null,[s(t,{to:"#let-s-get-started"},{default:a(()=>[e("Let's Get Started!")]),_:1}),n("ul",null,[n("li",null,[s(t,{to:"#understanding-vector-store-indexes"},{default:a(()=>[e("Understanding Vector Store Indexes")]),_:1})])])]),n("li",null,[s(t,{to:"#how-to-fine-tune-the-pipeline"},{default:a(()=>[e("How to Fine-Tune the Pipeline")]),_:1}),n("ul",null,[n("li",null,[s(t,{to:"#how-to-evaluate-the-pipeline-s-performance"},{default:a(()=>[e("How to Evaluate the Pipeline's Performance")]),_:1})]),n("li",null,[s(t,{to:"#iterate-on-the-index-structure-embedding-model-and-language-model"},{default:a(()=>[e("Iterate on the Index Structure, Embedding Model, and Language Model")]),_:1})]),n("li",null,[s(t,{to:"#experiment-with-different-hyperparameters"},{default:a(()=>[e("Experiment with Different Hyperparameters")]),_:1})])])]),n("li",null,[s(t,{to:"#real-world-applications-of-rag"},{default:a(()=>[e("Real-World Applications of RAG")]),_:1})]),n("li",null,[s(t,{to:"#rag-best-practices-and-considerations"},{default:a(()=>[e("RAG Best Practices and Considerations")]),_:1})]),n("li",null,[s(t,{to:"#conclusion"},{default:a(()=>[e("Conclusion")]),_:1}),n("ul",null,[n("li",null,[s(t,{to:"#key-points-to-remember"},{default:a(()=>[e("Key points to remember:")]),_:1})])])])])]),L,n("p",null,[e("This diagram is inspired by "),n("a",G,[s(o,{icon:"fas fa-globe"}),e("this article")]),e(" Let's go through the key pieces.")]),P,s(m,{id:"91",data:[{id:"Retrieval Component"},{id:"Generation Component"}],active:0},{title0:a(({value:r,isActive:p})=>[e("Retrieval Component")]),title1:a(({value:r,isActive:p})=>[e("Generation Component")]),tab0:a(({value:r,isActive:p})=>[S,C]),tab1:a(({value:r,isActive:p})=>[D]),_:1}),B,n("p",null,[e("We will be using Python and "),n("a",E,[s(o,{icon:"iconfont icon-ibm"}),e("IBM watsonx")]),e("> via LlamaIndex in this article. You should have the following on your system before getting started:")]),n("ul",null,[M,n("li",null,[n("a",W,[s(o,{icon:"iconfont icon-ibm"}),e("IBM watsonx project and API key")])]),H]),j,n("p",null,[e("To learn about how to get your watsonx.ai API keys, click "),n("a",z,[s(o,{icon:"iconfont icon-ibm"}),e("here")]),e('. You need the project ID and API Key to be able to work on the "Generation" aspect of RAG. Having them will help you make LLM calls through watsonx.ai.')]),O])}const K=h(y,[["render",V],["__file","how-to-build-a-rag-pipeline-with-llamaindex.html.vue"]]),F=JSON.parse(`{"path":"/freecodecamp.org/how-to-build-a-rag-pipeline-with-llamaindex.html","title":"How to Build a RAG Pipeline with LlamaIndex","lang":"ko-KR","frontmatter":{"lang":"ko-KR","title":"How to Build a RAG Pipeline with LlamaIndex","description":"Article(s) > How to Build a RAG Pipeline with LlamaIndex","icon":"fa-brands fa-meta","category":["AI","Meta","Llama","Article(s)"],"tag":["blog","freecodecamp.org","ai","meta","llm","llama","youtube","crashcourse"],"head":[[{"meta":null},{"property":"og:title","content":"Article(s) > How to Build a RAG Pipeline with LlamaIndex"},{"property":"og:description","content":"How to Build a RAG Pipeline with LlamaIndex"},{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/freecodecamp.org/how-to-build-a-rag-pipeline-with-llamaindex.html"}],["meta",{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/freecodecamp.org/how-to-build-a-rag-pipeline-with-llamaindex.html"}],["meta",{"property":"og:site_name","content":"📚Bookshelf"}],["meta",{"property":"og:title","content":"How to Build a RAG Pipeline with LlamaIndex"}],["meta",{"property":"og:description","content":"Article(s) > How to Build a RAG Pipeline with LlamaIndex"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://cdn.hashnode.com/res/hashnode/image/upload/v1725024307257/62401eea-25ab-4f00-93d7-76d7c49cf330.jpeg"}],["meta",{"property":"og:locale","content":"ko-KR"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://cdn.hashnode.com/res/hashnode/image/upload/v1725024307257/62401eea-25ab-4f00-93d7-76d7c49cf330.jpeg"}],["meta",{"name":"twitter:image:alt","content":"How to Build a RAG Pipeline with LlamaIndex"}],["meta",{"property":"article:tag","content":"blog"}],["meta",{"property":"article:tag","content":"freecodecamp.org"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:tag","content":"meta"}],["meta",{"property":"article:tag","content":"llm"}],["meta",{"property":"article:tag","content":"llama"}],["meta",{"property":"article:tag","content":"youtube"}],["meta",{"property":"article:tag","content":"crashcourse"}],["meta",{"property":"article:published_time","content":"2024-08-30T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"How to Build a RAG Pipeline with LlamaIndex\\",\\"image\\":[\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1724944925051/e525c6cb-6a99-4eec-8b47-3dc827ddff25.png\\"],\\"datePublished\\":\\"2024-08-30T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[]}"]],"prev":"/ai/llama/articles/README.md","date":"2024-08-30T00:00:00.000Z","isOriginal":false,"cover":"https://cdn.hashnode.com/res/hashnode/image/upload/v1725024307257/62401eea-25ab-4f00-93d7-76d7c49cf330.jpeg"},"headers":[{"level":2,"title":"Table of Contents","slug":"table-of-contents","link":"#table-of-contents","children":[]},{"level":2,"title":"What is Retrieval Augmented Generation (RAG)?","slug":"what-is-retrieval-augmented-generation-rag","link":"#what-is-retrieval-augmented-generation-rag","children":[{"level":3,"title":"Why use LlamaIndex for RAG?","slug":"why-use-llamaindex-for-rag","link":"#why-use-llamaindex-for-rag","children":[]},{"level":3,"title":"Some of the key benefits of using Llama-Index include:","slug":"some-of-the-key-benefits-of-using-llama-index-include","link":"#some-of-the-key-benefits-of-using-llama-index-include","children":[]},{"level":3,"title":"What You'll Learn Here:","slug":"what-you-ll-learn-here","link":"#what-you-ll-learn-here","children":[]}]},{"level":2,"title":"Understanding the Components of a RAG Pipeline","slug":"understanding-the-components-of-a-rag-pipeline","link":"#understanding-the-components-of-a-rag-pipeline","children":[{"level":3,"title":"Components of RAG","slug":"components-of-rag","link":"#components-of-rag","children":[]},{"level":3,"title":"The RAG Flow","slug":"the-rag-flow","link":"#the-rag-flow","children":[]},{"level":3,"title":"LamaIndex","slug":"lamaindex","link":"#lamaindex","children":[]}]},{"level":2,"title":"Prerequisites","slug":"prerequisites","link":"#prerequisites","children":[]},{"level":2,"title":"Let's Get Started!","slug":"let-s-get-started","link":"#let-s-get-started","children":[{"level":3,"title":"Understanding Vector Store Indexes","slug":"understanding-vector-store-indexes","link":"#understanding-vector-store-indexes","children":[]}]},{"level":2,"title":"How to Fine-Tune the Pipeline","slug":"how-to-fine-tune-the-pipeline","link":"#how-to-fine-tune-the-pipeline","children":[{"level":3,"title":"How to Evaluate the Pipeline's Performance","slug":"how-to-evaluate-the-pipeline-s-performance","link":"#how-to-evaluate-the-pipeline-s-performance","children":[]},{"level":3,"title":"Iterate on the Index Structure, Embedding Model, and Language Model","slug":"iterate-on-the-index-structure-embedding-model-and-language-model","link":"#iterate-on-the-index-structure-embedding-model-and-language-model","children":[]},{"level":3,"title":"Experiment with Different Hyperparameters","slug":"experiment-with-different-hyperparameters","link":"#experiment-with-different-hyperparameters","children":[]}]},{"level":2,"title":"Real-World Applications of RAG","slug":"real-world-applications-of-rag","link":"#real-world-applications-of-rag","children":[]},{"level":2,"title":"RAG Best Practices and Considerations","slug":"rag-best-practices-and-considerations","link":"#rag-best-practices-and-considerations","children":[]},{"level":2,"title":"Conclusion","slug":"conclusion","link":"#conclusion","children":[{"level":3,"title":"Key points to remember:","slug":"key-points-to-remember","link":"#key-points-to-remember","children":[]}]}],"git":{"contributors":[{"name":"chanhi2000","email":"chanhi2000@gmail.com","commits":2}]},"readingTime":{"minutes":9.75,"words":2924},"filePathRelative":"freecodecamp.org/how-to-build-a-rag-pipeline-with-llamaindex.md","localizedDate":"2024년 8월 30일","excerpt":"\\n"}`);export{K as comp,F as data};
